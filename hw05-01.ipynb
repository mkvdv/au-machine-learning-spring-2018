{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, HW 05!\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "print(\"Hello, HW 05!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)  Напишем, что у нас пока есть.\n",
    "Softmax: $ x^L_j = \\sigma ^L(s^L_j) = \\frac{e^{s^L_j}}{ \\sum_{i}^{} e^{s^L_i} }$  \n",
    "Cross-Entropy : $  C(w) = - \\sum_{t}^{}o_t \\log{x^L_t} $  \n",
    "\n",
    "$ \\delta^L_j = \\frac{ d\\ C(w)}{d\\ s^L_j} =\n",
    "\\sum_{i} \\frac{ d\\ C(w)}{d\\ x^L_i} \\frac{ d\\ x^L_i}{d\\ s^L_j}  = \n",
    "- \\sum_{i} \\frac{ d\\ \\sum_{t}^{}o_t \\log{x^L_t} }{d\\ x^L_i} \\frac{ d\\ x^L_i}{d\\ s^L_j}\n",
    "$  \n",
    "\n",
    "Сначала правую - выпишем производную softmax-a -- она разная в зависимости от того, равны ли i и j:  \n",
    "если равны, то \n",
    "$$ \\frac{ d\\ \\frac{e^{s^L_i}}{ \\sum_{k}^{} e^{s^L_k} }}{d\\ s^L_j} =  \n",
    " \\frac{  e^{s^L_i}  \\sum_{k}^{} e^{s^L_k} - e^{s^L_j} e^{s^L_i} }{ (\\sum_{k}^{} e^{s^L_k})^2  } =\n",
    "\\frac{  e^{s^L_i}   }\n",
    "{ \\sum_{k}^{} e^{s^L_k}  }\n",
    "\\frac{   \\sum_{k}^{} e^{s^L_k} - e^{s^L_j} }\n",
    "{ \\sum_{k}^{} e^{s^L_k}  } =\n",
    "x^L_i(1 - x^L_j)\n",
    "$$ \n",
    "\n",
    "Если же не равны, то  \n",
    "$$ \\frac{ d\\ \\frac{e^{s^L_i}}{ \\sum_{k}^{} e^{s^L_k} }}{d\\ s^L_j} =  \n",
    "\\frac{  0 - e^{s^L_j} e^{s^L_i} }{ (\\sum_{k}^{} e^{s^L_k})^2  } =\n",
    "    \\frac{  -e^{s^L_j}   }\n",
    "        { \\sum_{k}^{} e^{s^L_k}  }\n",
    "    \\frac{ e^{s^L_i} }\n",
    "        { \\sum_{k}^{} e^{s^L_k}  } =\n",
    "-x^L_j x^L_i\n",
    "$$ \n",
    "\n",
    "То есть\n",
    "$$ \\frac{ d\\ x^L_i}{d\\ s^L_j} = x^L_i(\\delta_{ij} - x^L_j) $$  \n",
    ", где $\\delta_{ij}$ - символ Кроненкера.  \n",
    "\n",
    "\n",
    "Теперь производная кросс-ентропии:  \n",
    "\n",
    "$$ \\frac{ d\\ \\sum_{t}^{}o_t \\log{x^L_t} } {d\\ x^L_i} =   \n",
    "         o_i \\frac{1}{x^L_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Тогда исходное выржение :  \n",
    "$$ \\delta^L_j = \\frac{ d\\ C(w)}{d\\ s^L_j} =\n",
    "\\sum_{i} \\frac{ d\\ C(w)}{d\\ x^L_i} \\frac{ d\\ x^L_i}{d\\ s^L_j}  = \n",
    "- \\sum_{i} \\frac{ d\\ \\sum_{t}^{}o_t \\log{x^L_t} }{d\\ x^L_i} \\frac{ d\\ x^L_i}{d\\ s^L_j} = \\\\\n",
    "- \\sum_{i} \\frac{o_i}{x^L_i}  x^L_i(\\delta_{ij} - x^L_j) =\n",
    "- \\sum_{i} o_i (\\delta_{ij} - x^L_j) = \n",
    "\\sum_{i} o_i ( x^L_j - \\delta_{ij})\n",
    "$$\n",
    "\n",
    "В интернетах как-то дальше упрощают формулу, но у них вместо $o_i$ появляется $y_i$ и они там несколько хаков делают, и получается вообще $x^L_j - y_j$, но не ясно, откуда они $y_i$ получили. Кажется, что полученное выражение вполне себе в общем виде и его можно быстро посчитать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
